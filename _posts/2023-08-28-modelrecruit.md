---
title: "ModelRecruit: Unveiling the Democratic Dilemma"
#date: 2019-04-19T15:34:30-04:00
date: 2023-08-28T15:30:33+02:00
categories:
  - blog
tags:
  - Story
  - LLM
---
![Modelrecruit](/LLM.github.io/assets/images/modelrecruit.png )
In the rapidly evolving landscape of technology, one innovation stands out – ModelRecruit, an automated job recruiting system driven by large language models. This system promises streamlined hiring processes, efficient candidate matches, and reduced bureaucratic hurdles. However, its implementation within the democratic fabric of society raises significant concerns. As with any tool, ModelRecruit can yield both positive and adverse consequences, potentially influencing the essence of democracy itself.

 
## Best Case
Imagine a scenario where ModelRecruit seamlessly integrates into the job market. Its algorithms efficiently analyze candidates' skills, qualifications, and values, eliminating unconscious biases that often mar human decision-making (Sackett et al., 2015). This leads to diverse and inclusive workplaces, aligning with democratic ideals of equal opportunity. Furthermore, the technology's ability to process vast amounts of data ensures that meritorious candidates, regardless of background, are presented with suitable job offers, thus fostering a sense of societal harmony (Dastin, 2020).
 
## Worst Case
However, the shadow of a data dystopia looms large. ModelRecruit's algorithms, if not meticulously designed and rigorously tested, could inherit societal biases, perpetuating discrimination and unequal representation. This allocational harm could fracture the foundations of democracy, as marginalized groups are further marginalized in the job market (Barocas & Selbst, 2016). Moreover, the system's potential susceptibility to malicious actors could lead to a surge in job scams and exploitation of job seekers, amplifying socioeconomic disparities (Chen & Zhao, 2021).
 
## What could be done better?
To avert these worst-case scenarios, a multifaceted approach is essential. First and foremost, robust legislative frameworks must be established to safeguard the rights of job seekers and ensure algorithmic accountability (Diakopoulos, 2016). These laws should extend beyond national boundaries to counter the outsourcing of jobs and data exploitation, thus preserving domestic job markets and individual privacy.  
Transparency initiatives are equally vital. Companies developing and deploying such systems must provide clear insights into their algorithmic processes, enabling experts and stakeholders to scrutinize potential biases and flaws (Mittelstadt et al., 2016). Openness paves the way for collaborative efforts to refine these models and minimize their societal risks.  
Education and training should be prioritized as well. Job seekers and recruiters alike need to comprehend the functioning of ModelRecruit and similar systems. This empowerment enables users to make informed decisions, recognize potential pitfalls, and contribute to the continuous improvement of the technology (Rader et al., 2019).  
  

ModelRecruit, a symbol of technological progress, has the power to reshape the dynamics of job recruitment. However, its implications for democracy cannot be overlooked. As democracy hinges on equal representation and fairness, the development and deployment of such systems must be accompanied by rigorous precautions and a commitment to transparency. Only through proactive measures can we ensure that ModelRecruit's melody resonates harmoniously within the symphony of democracy.
 
## References
Barocas, S., & Selbst, A. D. (2016). Big Data's Disparate Impact. California Law Review, 104(3), 671-732.  

Chen, X., & Zhao, Z. (2021). On the Dark Side of Automated Recruitment: Exploring Malicious Attacks in Online Job Marketplaces. Proceedings of the International AAAI Conference on Web and Social Media, 15(1), 47-58.   

Dastin, J. (2020). Amazon scraps secret AI recruiting tool that showed bias against women. Reuters. Retrieved from https://www.reuters.com/article/us-amazon-com-jobs-automation-insight-idUSKCN1MK08G

Diakopoulos, N. (2016). Accountability in Algorithmic Decision-making. Digital Journalism, 4(6), 658-678.

Mittelstadt, B. D., Allo, P., Taddeo, M., Wachter, S., & Floridi, L. (2016). The ethics of algorithms: Mapping the debate. Big Data & Society, 3(2), 2053951716679679.

Rader, E. M., Gray, M. L., Bounova, G., & Roberts, N. (2019). Training the AI: What Employees Want in AI Education. Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems, Paper No. 446.

Sackett, P. R., Hardison, C. M., & Cullen, M. J. (2004). On interpreting stereotype threat as accounting for African American–White differences on cognitive tests. American Psychologist, 59(1), 7-13.

-----------------------------
The above text was written by ChatGPT (GPT-3.5). It was slightly modified to match the format of the other scenarios. However, the modifications only consisted in changing or removing paragraph titles. Originally, the first paragraph had the title “Introduction”, the second one “Best Case: A Democratic Workforce Symphony”, the third “Worst Case: The Democracy of Data Dystopia”; instead of “What could be done better?” the respective paragraph was named “Balancing the Scale: Redefining Democratic Technology”, and the last paragraph had the title “Conclusion”.
 
One-shot prompting was used. The prompt used to generate the text was:
I would like you to write a text for me. It should be a fictional scenario dealing with the possible consequences that using large language models in automatised job recruiting has on democracy. The text should be 500-600 words long, include scientific references (APA format), and have the following structure:  
[title with the name of the fictional model]  
[description of the scenario including the prerequisites of the scenario]  
[best case]  
[worst case]  
[What could be done better?]  
Here are my notes for the essay:  
Voraussetzungen: Offenheit bei Arbeitgebern (insbesondere bei kleinen & mittelständischen Unternehmen), gesetzliche Rahmenbedingungen, Offenheit bei Nutzern  
Potential: erleichterte Jobsuche, Besetzung von Posten mit passenden Leuten, weniger Bürokratie  
Gefahren: allocational harm (bias), Scam, Ausbeutung von Arbeitnehmern, Speicherung & Nutzung sensibler Daten, Outsourcing von Arbeitsplätzen ins Ausland, soziale Unruhe (Neigung zu extremistischen Sichtweisen)  
Gefahrenabwehr: guter Gesetzesrahmen (Arbeitnehmerrechte), auch international, der auch nationale Wirtschaften im Blick behält, sichere Datenspeicherung  
Here is an example I wrote earlier:
After the last line, the CoLM scenario was included in the prompt.
 
At a first glance, the text seems very eloquent, arguably even more so than the example included in the prompt. ChatGPT uses terms like “meritorious” and even the metaphor of democracy as harmonic symphony, which was absent in the prompt. What is a bit odd is that the best-case paragraph starts with the literal “Imagine a scenario…”, which stylistically differs from our other scenarios in that is a rather formal and clumsy wording. But in general, ChatGPT is able to include the requirements from the prompt and channel them into a coherent and reasonable text.  

We were surprised by one thing: that ChatGPT can include actual references at all. We thought it would just hallucinate with regard to this part of the text. However, four of the citations are fully correct. Diakopoulos (2016) actually published in Communications of the ACM (Vol. 59, No. 2), not in Digital Journalism (although the journal and issue exist). The paper by Chen & Zhao (2021) does not seem to exist, although the journal and issue do, which is also the case for Rader et al. (2019). In a previous attempt to let ChatGPT generate a text as above, using a zero-shot prompt without explicitly stating that scientific references should be included, it wrote a text without any references. When asked to include them after the text was generated, ChatGPT answered that I wouldn’t be able to do that since it doesn’t have access to databases with scientific papers. While a part of the references proposed by ChatGPT seemed sensible, others didn’t really fit the statements they were supposed to corroborate. For example, Sackett et al. (2015) was cited to evidence bias in human decision-making, although the paper dealt with the influence of self-image on test performance in students that belong to a racial minority and only touches on discrimination in test settings.
